\chapteruaf{Common Analysis Tools}
\section{Introduction}
In this chapter we will be discussing a number of statistical terms and methods which will be common to the remainder of this dissertation. 

\section{Definitions and Terms}
We begin our discussion by defining the terms population and sample. A population is the complete set of data that have one property in common that is the subject of some statistical analysis. A sample is a subset selected from a larger population~\cite{devore_probability_2011}. For example we might consider a population as all people over 1.8m tall and a sample could be 1,000 randomly chosen people over 1.8m tall. 

Next let us consider a sample $X$ drawn from some population. Further let us define an individual element in $X$ as $x_i$ where $ \{x_1, x_2, ....., x_{n-1}, x_n \} \in X$. Next we assume that each $x_i$ has a probability of occurrence of $p_i$. Further we assume that all values in $X$ are independent and identically distributed (iid). That is, no element in X depends on another and all elements are pulled from the same distribution.  We define the mean of $X$ as~\cite{wackerly_mathematical_2007} 

 \begin{equation}\label{mean}
 	\mu=\frac{\sum_{i=1}^{n} x_i}{n}
 \end{equation} 

This represents the ``average'' value of the dataset. We further define the standard deviation of the dataset ~\cite{wackerly_mathematical_2007} as 

\begin{equation}\label{sigma}
	\sigma = \sqrt{\sum_{i=1}^{n}p_i(x_i-\mu)^2}
\end{equation}

The standard deviation gives a measure of how dispersed the sample is from the average. Finally we define the define the variance of $X$ which is simply $\sigma^2$ and gives a measure of how spread out the sample is. 

\section{Hypothesis Testing}
In this test we will be using statistical hypothesis testing in order to analyze our data. Hypothesis testing is the process of using statistical test to determine whether a hypothesis about some model is false. We begin by defining the null hypothesis $H_0$ as some claim we initially assume to be true ~\cite{devore_probability_2011}. The alternate hypothesis $H_a$ is the claim we assume to be true if we reject $H_0$. 

When discussing hypothesis testing we will be comparing two random samples. The first sample is $X$ where  $X_i$ has a probability of  $p_{xi}$ and $\{x_1,x_2,...,x_{n_x-1} , x_{n_x}\} \in X$ . The second sample is $Y$ where  $Y_i$ has a probability of  $p_{yi}$ and $\{y_1,y_2,...,y_{n_y-1} , y_{n_y}\} \in Y$ .

Using an appropriate statistical test (discussed in sections ~\ref{sec:ttest} and ~\ref{sec:mannWhitney}) we will be able to compute our test statistic. A test statistic a the result of a function of our data which gives us one number upon which we can base our rejection of $H_0$. Given the test statistic we can then compute our $p$-value. The $p$-value is the probability ``calculated assuming $H_0$ is true of obtaining a test statistic at least as contradictory to $H_0$ as the value actually computed''~\cite{devore_probability_2011}. That is, a $p$-value gives us the probability that our test statistic would have been produced if $H_0$ were true. The smaller the $p$-value the more the data contradicts $H_0$. Note this is not the same as saying the $p$-value is the probability that $H_0$ is true nor is it the error associated with our test. Next we have our critical region which is the set of values for which we reject $H_0$. If our $p$-value falls in our critical region we reject $H_0$.

When we discuss hypothesis testing we must also discuss the types of errors associated with it. A type \Romannum{1} error is when one asserts that $H_0$ is true when it is in fact false. This is also called a false positive. A type \Romannum{2} error is when the null is false but is not rejected. This is also called a false negative. For brevity we will call the number of true positives generated by some process as $TP$, the number of true negatives as $TN$, the number of false positives as $FP$, and the number of false negatives as $FN$. 

We further define the term accuracy as 

\begin{equation}\label{Accuracy}
	accuracy=\frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

Accuracy gives us a measure of how close to the ``correct'' result the predicted results are. This will serve our metric for determining how good a classification is. 
\subsection{Welch's t-test}\label{sec:ttest}

The first hypothesis which will be used is Welch's t-test\cite{welch_generalization_1947}. In Welch's $t$-test we look at the two populations and calculate a $t$-statistic. This is more robust for our purposes than the more common student $t$-test as it does not assume both samples $X$ and $Y$ have the same variance. Based on this statistic we can determine whether or not to reject the null hypothesis.  

We next compute the t-statistic for the two populations $X$ and $Y$ via the following formula:

	 \begin{equation}\label{t-stat}
		t = \frac{  \mu_x - \mu_y }{ \frac{\sigma_{x}^2}{n_x} + \frac{\sigma_{y}^2}{n_y}   }
	 \end{equation}

Now that we have the $t$-statistic we can begin to compute the $p$-value. First however we need to determine the degrees of freedom $\nu$ for our test. For each of our random samples $X$ and $Y$ the degrees of freedom are given by $\nu_x = n_x -1$ and $\nu_y = n_y-1$. We can then approximate the degrees of freedom for Welch's $t$-test using the Welch-Satterthwaite equation ~\cite{satterthwaite1946approximate,welch_generalization_1947}

	\begin{equation}\label{tTestDegrees}
		\nu \approx \frac{ (\frac{\sigma_{x}^2}{n_{x}^2}+\frac{\sigma_{y}^2}{n_{y}^2})^2 }{\frac{\sigma_{x}^4}{n_{x}^2 \nu_x}+\frac{\sigma_{y}^4}{n_{y}^2 \nu_y}}
	\end{equation}

With the degrees of freedom for the test we can compute finally compute the $p$-value by using the probability density function (pdf) for student's $t$-distribution. 

\begin{equation}\label{tTestPDF}
	f(t)=\frac{\Gamma(\frac{\nu+1}{2})}{\sqrt{\nu}\Gamma(\frac{\nu}{2})}(1+\frac{t^2}{2})^{\frac{-\nu+1}{2}}
\end{equation}

where 

\begin{equation}\label{Gamma}
	\Gamma(a) = \int_{0}^{\infty} x^{a-1}e^{x}dx
\end{equation}

We now obtain the $p$-value by integrating the pdf of student's $t$-distribution from $t$ to $\infty$. 

\begin{equation}\label{pvalStudentTest}
	p=\int_{t}^{\infty}f(t)dt 
\end{equation}

With the p-value in hand we can determine whether or not to reject the null hypothesis by selecting a critical value. If the p-value is smaller than that critical value then we can reject the null hypothesis that the two distributions share a mean.  For all t-tests in this dissertation we will assume a critical value of 0.001. We will be performing all of our t-tests with the scipy library~\cite{jones_scipy:_2001}. It should be noted that many of the p-values obtained in this dissertation are 0. For a finite dataset it is impossible to obtain a p-value of 0, this instead a limitation of IEEE floating point arithmetic and should be taken as less than  . While these values may initially appear suspicious it should be noted they can easily be verified by performing the integral manually (or rather with a numerically using a different function in scipy~\cite{jones_scipy:_2001}).  It can also be intuitively verified if one looks at the student-t distribution for one degree of freedom (fig ~\ref{TDist}).  We see that as x increases P(x) goes to 0.  

\begin{figure}\label{TDist}
	  \centering
	  \includegraphics[width=3in]{figures/T_distribution_1df.eps}
	  \caption{t-distribution for one degree of freedom}
\end{figure}

\subsection{Mann-Whitney U Test}\label{sec:mannWhitney}

The Mann Whitney U-test ~\cite{wackerly_mathematical_2007} , also called the Wilcoxon rank-sum test, is a hypothesis test which tests the null hypothesis that one sample has tends to have larger values than another. We begin the Mann Whitney U-test using the same populations, means, and standard deviations as in Welch's t-test. Then we check to see whether or not the following four assumptions are satisfied. 

\begin{enumerate}
	\item $X$ and $Y$ are independent of each other
	\item All observations are ordinal (i.e. it can be distinguished that one observation is greater than another) 
	\item Under the null hypothesis both groups are equal
	\item Under the alternate hypothesis the probability of an observation from one population exceeding an observation from the other is not 0.5. 
\end{enumerate}

In order to perform the test we compute the $U$ statistic.  There are two methods for computing the $U$ statistic: one for small data sets (less than 20 elements) and one for larger data sets.  As all of the datasets used in this are on the order of $10^6$ elements we will only be discussing the latter.

The first step is to rank all the observations. The smallest observation is assigned the rank 1, the next smallest value is assigned the rank 2, and so on.  Ties are equal to the midpoint of the assigned rankings. So in the set $\{2,4,4,7\}$ the ranks $\{1, 2.5, 2.5, 4\}$ would be assigned. 

The next step is to sum the ranks of all observations taken from $X$ and call it $R_1$.  The sum of the ranks from observations taken from $Y$ is given by $R_2$. We can then then compute the $U$ statistics 

\begin{equation}\label{U1}
	U_1=n_1 n_2 + \frac{n_1(n_1+1)}{2} - R_1
\end{equation}

\begin{equation}\label{U2}
	U_2=n_1 n_2 + \frac{n_2(n_2+1)}{2} - R_2
\end{equation}

The smaller of $U_1$and $U_2$ is chosen for computing the p-value which are obtained through a table of critical values. As with the t-test we will be using the scipy implementation ~\cite{jones_scipy:_2001} for our Mann Whitney U-tests. 

\section{Mutual Information}

Mutual information is a quantity which measures the amount of certainty gained about a population $X$ when measuring some random variable $Y$.  In particular let $Y\in {0,1}$ represent whether or not the VM is monitored by some VMI agent and let $X\in \mathbb{R}^n $represent the measured data from a given experiment.  The mutual information   then specifies how many bits of information are gained about $Y$ when sampling the random variable $X$.  Given a joint probability distribution $P(Y=y,X=x)$  the mutual information is given by 

\begin{equation}\label{MutInformEq}
	I(Y:X) = \sum_{x\in X}\sum_{y\in Y} p(y,x)lg(\frac{p(y,x)}{p(x)p(y)})
\end{equation}

To estimate the mutual information, we use the implementation provided by scikit-learn~\cite{pedregosa_scikit-learn:_2011-1} where $X$ is approximated by histograms over a large number of sample measurements. Mutual information will allow us to determine how much many samples are needed in order to make a classification between two samples. 